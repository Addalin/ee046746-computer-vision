{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"https://img.icons8.com/bubbles/100/000000/3d-glasses.png\" style=\"height:50px;display:inline\"> EE 046746 - Technion - Computer Vision\n",
    "\n",
    "#### Dahlia Urbach\n",
    "\n",
    "## Tutorial 09 - Introduction to 3D Deep Learning\n",
    "---\n",
    "<img src=\"./assets/tut_09_teaser.JPG\" style=\"width:800px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [Depth Cameras - Quick overview](#)\n",
    "    * Stereo Cameras - Next Week\n",
    "    * [Time of Flight](#)\n",
    "* [3D Deep Learning](#)\n",
    "    * Voxels\n",
    "    * Multi-View\n",
    "    * Point Clouds\n",
    "* [3D Applications](#)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Recommended Tools](#-Recommended-Tools)\n",
    "\n",
    "* [Credits](#-Credits)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_resolve_type_from_object(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: object, arg1: torch._C._jit_tree_views.SourceRange, arg2: Callable[[str], function]) -> torch._C.Type\n\nInvoked with: typing.Union[int, NoneType], None, <function try_ann_to_type.<locals>.fake_rcb at 0x000001BF33ACB2F0>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-e9154bdbe479>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDataLoader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mDataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mConcatDataset\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtorchvision\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torchvision\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mwarnings\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchvision\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torchvision\\models\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mshufflenetv2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msegmentation\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdetection\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mvideo\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mquantization\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torchvision\\models\\detection\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mfaster_rcnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mmask_rcnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mkeypoint_rcnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torchvision\\models\\detection\\faster_rcnn.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mgeneralized_rcnn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGeneralizedRCNN\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mrpn\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAnchorGenerator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRPNHead\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mRegionProposalNetwork\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mroi_heads\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mRoIHeads\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mGeneralizedRCNNTransform\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mbackbone_utils\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mresnet_fpn_backbone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    208\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 210\u001B[1;33m \u001B[1;33m@\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscript\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    211\u001B[0m def _onnx_heatmaps_to_keypoints_loop(maps, rois, widths_ceil, heights_ceil,\n\u001B[0;32m    212\u001B[0m                                      widths, heights, offset_x, offset_y, num_keypoints):\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\__init__.py\u001B[0m in \u001B[0;36mscript\u001B[1;34m(obj, optimize, _frames_up, _rcb)\u001B[0m\n\u001B[0;32m   1288\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_rcb\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1289\u001B[0m             \u001B[0m_rcb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_jit_internal\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreateResolutionCallbackFromClosure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1290\u001B[1;33m         \u001B[0mfn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_script_compile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mqualified_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mast\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_rcb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mget_default_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1291\u001B[0m         \u001B[1;31m# Forward docstrings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1292\u001B[0m         \u001B[0mfn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__doc__\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\_recursive.py\u001B[0m in \u001B[0;36mtry_compile_fn\u001B[1;34m(fn, loc)\u001B[0m\n\u001B[0;32m    566\u001B[0m     \u001B[1;31m# object\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    567\u001B[0m     \u001B[0mrcb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_jit_internal\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreateResolutionCallbackFromClosure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 568\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mscript\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_rcb\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrcb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    569\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    570\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mwrap_cpp_module\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcpp_module\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\__init__.py\u001B[0m in \u001B[0;36mscript\u001B[1;34m(obj, optimize, _frames_up, _rcb)\u001B[0m\n\u001B[0;32m   1288\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_rcb\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1289\u001B[0m             \u001B[0m_rcb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_jit_internal\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcreateResolutionCallbackFromClosure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1290\u001B[1;33m         \u001B[0mfn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_script_compile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mqualified_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mast\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_rcb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mget_default_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1291\u001B[0m         \u001B[1;31m# Forward docstrings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1292\u001B[0m         \u001B[0mfn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__doc__\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__doc__\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\__init__.py\u001B[0m in \u001B[0;36m_get_overloads\u001B[1;34m(obj)\u001B[0m\n\u001B[0;32m   2028\u001B[0m     \u001B[0mcompiled_fns\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2029\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0moverload_fn\u001B[0m \u001B[1;32min\u001B[0m \u001B[0muncompiled_overloads\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2030\u001B[1;33m         \u001B[0mcompiled_fns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_compile_function_with_overload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moverload_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mqual_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2031\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2032\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mexisting_compiled_fns\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\__init__.py\u001B[0m in \u001B[0;36m_compile_function_with_overload\u001B[1;34m(overload_fn, qual_name, impl_fn)\u001B[0m\n\u001B[0;32m   2008\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_compile_function_with_overload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moverload_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mqual_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimpl_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2009\u001B[0m     \u001B[0moverload_decl\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_jit_def\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moverload_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecl\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2010\u001B[1;33m     \u001B[0moverload_signature\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mannotations\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_signature\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moverload_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minspect\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mismethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moverload_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2011\u001B[0m     \u001B[0mimpl_ast\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjit\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_jit_def\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimpl_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2012\u001B[0m     \u001B[0moverload_defaults\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_default_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moverload_fn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\annotations.py\u001B[0m in \u001B[0;36mget_signature\u001B[1;34m(fn, rcb, loc, is_method)\u001B[0m\n\u001B[0;32m     77\u001B[0m         \u001B[1;31m# because it didn't have any annotations.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtype_line\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 79\u001B[1;33m             \u001B[0msignature\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparse_type_line\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtype_line\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrcb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     80\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0msignature\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\annotations.py\u001B[0m in \u001B[0;36mparse_type_line\u001B[1;34m(type_line, rcb, loc)\u001B[0m\n\u001B[0;32m    163\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Failed to parse the return type of a type annotation: {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 165\u001B[1;33m     \u001B[0marg_types\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mann_to_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mann\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mann\u001B[0m \u001B[1;32min\u001B[0m \u001B[0marg_ann\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    166\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0marg_types\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mann_to_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mret_ann\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\annotations.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    163\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Failed to parse the return type of a type annotation: {}\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 165\u001B[1;33m     \u001B[0marg_types\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mann_to_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mann\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mann\u001B[0m \u001B[1;32min\u001B[0m \u001B[0marg_ann\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    166\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0marg_types\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mann_to_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mret_ann\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\annotations.py\u001B[0m in \u001B[0;36mann_to_type\u001B[1;34m(ann, loc)\u001B[0m\n\u001B[0;32m    301\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    302\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mann_to_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mann\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 303\u001B[1;33m     \u001B[0mthe_type\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtry_ann_to_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mann\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    304\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mthe_type\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    305\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mthe_type\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\dallu\\pycharmprojects\\cv2020\\venv\\lib\\site-packages\\torch\\jit\\annotations.py\u001B[0m in \u001B[0;36mtry_ann_to_type\u001B[1;34m(ann, loc)\u001B[0m\n\u001B[0;32m    294\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mfake_rcb\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    295\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 296\u001B[1;33m         \u001B[0mthe_type\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_resolve_type_from_object\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mann\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfake_rcb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    297\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mthe_type\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    298\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mthe_type\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: _resolve_type_from_object(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: object, arg1: torch._C._jit_tree_views.SourceRange, arg2: Callable[[str], function]) -> torch._C.Type\n\nInvoked with: typing.Union[int, NoneType], None, <function try_ann_to_type.<locals>.fake_rcb at 0x000001BF33ACB2F0>"
     ]
    }
   ],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=R-ZXdAEGbiw&feature=youtu.be\">LiDAR point cloud support | Feature Highlight | Unreal Engine\n",
    "</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/office/80/000000/depth.png\" style=\"height:50px;display:inline\"> Depth Cameras\n",
    "---\n",
    "* Stereo Cameras - Next week\n",
    "* Time of flight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/android/48/000000/time.png\" style=\"height:50px;display:inline\"> Time of Flight Cameras\n",
    "---\n",
    "* Light travels at approximately a constant speed $c = 3\\times 10^8$ (meters per second).\n",
    "* Measuring the time it takes for light to travel over a distance once can infer distance.\n",
    "* Can be categorized into two types:\n",
    "    1. Direct TOF - switch laser on and off rapidly.\n",
    "    2. Indirect TOF - send out modulated light, then measure phase difference to infer depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### 1. Direct - TOF\n",
    "* **Li**ght **D**etection **A**nd **R**anging (LiDAR) probably best example in computer vision and robotics.\n",
    "* High-energy light pulses limit influence of background illumination.\n",
    "* However, difficulty to generate short light pulses with fast rise and fall times.\n",
    "* High-accuracy time measurement required.\n",
    "* Prone to motion blur.\n",
    "* Sparser as objects grow in distance.\n",
    "<img src=\"./assets/tut_09_LiDAR.GIF\" style=\"width:150px\">\n",
    "<a href=\"https://en.wikipedia.org/wiki/Lidar\">Gif source - Wikipedia</a> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\".\\assets\\tut_09_sydney.png\" style=\"width:400px\">\n",
    "<a href=\"http://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml\">Sydney Dataset</a> \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Autonomous Car - LiDAR \n",
    "<img src=\".\\assets\\tut_09_cam1.JPG\" style=\"width:400px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### SLAM + LIDAT - Zebedee\n",
    "<img src=\".\\assets\\tut_09_cam2.JPG\" style=\"width:400px\">\n",
    "<a href=\"https://research.csiro.au/robotics/zebedee/\">zebedee</a> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### 2. Indirect - TOF\n",
    "* Continuous light waves instead of short light pulses.\n",
    "* Modulation in terms of frequency of sinusoidal waves.\n",
    "* Detected wave after reflection has shifted phase.\n",
    "* Phase shift proportional to distance from reflecting surface.\n",
    "\n",
    "<img src=\".\\assets\\tut_09_cam3.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\".\\assets\\tut_09_cam4.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/cloud.png\" style=\"height:50px;display:inline\"> Deep Learning on Point Clouds\n",
    "---\n",
    "<img src=\".\\assets\\tut_o9_pn1.JPG\" style=\"width:800px\">\n",
    "\n",
    "* Calssification\n",
    "* Semantic segmentation\n",
    "* Part segmentation\n",
    "    * Each point belongs to a specific part of the object\n",
    "* ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "*Qi, Charles R., et al. \"Pointnet: Deep learning on point sets for 3d classification and segmentation.\" CVPR. 2017.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/plasticine/100/000000/not-applicable.png\" style=\"height:50px;display:inline\"> Point Clouds Problems\n",
    "---\n",
    "* Point Clouds Vary in Size (not constant)\n",
    "* Unordered Input\n",
    "    * Data is unstructured (no grid)\n",
    "    * Data is invariant to point ordering (permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Other Point Clouds Challenges\n",
    "---\n",
    "* Missing data\n",
    "* Noise \n",
    "* Rotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Problem - Point Clouds Vary in Size\n",
    "---\n",
    "<img src=\".\\assets\\tut_09_pn2.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\".\\assets\\tut_09_pn3.JPG\" style=\"width:800px\">\n",
    "\n",
    "* Different point clouds represent the same object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem - Unordered Input - Unordered Input\n",
    "---\n",
    "# Tal: is the title correct?\n",
    "Point cloud: $N$ **orderless** points, each represented by a $D$ dim vector\n",
    "<img src=\".\\assets\\tut_09_pn4.JPG\" style=\"width:800px\">\n",
    "How many semi-equal representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Model needs to be invariant to $N!$ permutations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  <img src=\"https://img.icons8.com/carbon-copy/100/000000/switch-camera.png\" style=\"height:50px;display:inline\">Alternate 3D Representations - Representations\n",
    "---\n",
    "Solution:\n",
    "* Convert the raw point clouds into Voxels or multiple 2D RGB(D) images\n",
    "\n",
    "<img src=\".\\assets\\tut_09_pn5.JPG\" style=\"width:800px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another 3D representation (not in this course):\n",
    "<img src=\".\\assets\\tut_09_other.JPG\" style=\"width:800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/carbon-copy/100/000000/sugar-cubes.png\" style=\"height:50px;display:inline\"> Voxalization\n",
    "---\n",
    "Idea: generalize 2D convolutions to regular 3D grids\n",
    "\n",
    "* The straightforward approach: transform the point clouds into a voxel grid by rasterizing and use 3D CNNs\n",
    "<img src=\".\\assets\\tut_09_vox1.jpg\" style=\"width:200px\">\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3D convolution uses 4D kernels\n",
    "<img src=\".\\assets\\tut_09_vox2.JPG\" style=\"width:600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\".\\assets\\tut_09_voxnet.png\" style=\"width:500px\">\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Maturana, Daniel, and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition. IROS, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/plasticine/100/000000/not-applicable.png\" style=\"height:50px;display:inline\"> Voxalization Problems\n",
    "---\n",
    "* Large memory cost\n",
    "* Slow processing time\n",
    "* Limited spatial resolution\n",
    "* Quantization artifacts\n",
    "<img src=\".\\assets\\tut_09_vox3.JPG\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/multiple-cameras.png\" style=\"height:50px;display:inline\"> Multi-View Approach\n",
    "---\n",
    "Idea: Transfrom the problem into a well known domain (3D$\\rightarrow$2D)\n",
    "\n",
    "* The multi-view approach: project multiple views to 2D and use CNN to process\n",
    "    * How many views do we need? (Another hyper parameter)\n",
    "    \n",
    "<img src=\".\\assets\\tut_09_pn7.png\" style=\"width:600px\">\n",
    "\n",
    "H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller. Multiviewconvolutional neural networks for 3d shape recognition. CVPR, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/doodle/48/000000/direction-sign.png\" style=\"height:50px;display:inline\"> Apply Deep Learning Directly on 3D Point Clouds\n",
    "---\n",
    "Idea: Most of the raw 3D data are point clouds - Solve the problems!\n",
    "\n",
    "Note: Point Clouds Problems:\n",
    "* Point Clouds Vary in Size (not constant)\n",
    "* Unordered Input\n",
    "    * Data is unstructured (no grid)\n",
    "    * Data is invariant to point ordering (permutations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cute-clipart/64/000000/machine-learning.png\" style=\"height:50px;display:inline\"> PointNet \n",
    "---\n",
    "#### Permutation Invariance: Symetric Function\n",
    "$$ f(x_1,x_2,\\dots,x_n) \\equiv f(x_{\\pi_1},x_{\\pi_2},\\dots,x_{\\pi_n}), x_i \\in R^D $$\n",
    "\n",
    "$\\pi$ is a different permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example:\n",
    "$$ f(x_1,x_2,\\dots,x_n) = max\\{x_1,x_2,\\dots,x_n\\}$$\n",
    "$$ f(x_1,x_2,\\dots,x_n) = x_1+x_2+\\dots+x_n$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* How can we construct a family of symmetric functions by neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Observe:\n",
    "$$ f(x_1,x_2,\\dots,x_n) = \\gamma \\circ g(h(x_1),\\dots,h(x_n))$$ \n",
    "is symmetric if $g$ is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\".\\assets\\tut_09_pn8.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/wired/64/000000/diversity.png\" style=\"height:50px;display:inline\"> Basic PointNet Architecture\n",
    "---\n",
    "Empirically, we use multi-layer perceptron (MLP) and max pooling:\n",
    "\n",
    "<img src=\".\\assets\\tut_09_pn9.JPG\" style=\"width:500px\">\n",
    "\n",
    "Input MLP:\n",
    "$$h(x_i): R^{3} \\rightarrow{} R^{D}$$\n",
    "We can look at it as $D$ functions $\\{h_k\\}_{k=1}^D$ operate on each point where $$h_k(x_i): R^{3} \\rightarrow{} R^{1}$$\n",
    "\n",
    "Pooling layer:\n",
    "$$g(h(x_1),\\dots,h(x_n)): R^{N\\times D} \\rightarrow{} R^{D}$$\n",
    "We apply the pooling over all points for each function $h_k$. \n",
    "$$g(h_k(x_1),\\dots,h_k(x_n)): R^{N\\times 1} \\rightarrow{} R^{1}$$\n",
    "\n",
    "Classification MLP:\n",
    "$$\\gamma \\circ g(h(x_1),\\dots,h(x_n)): R^{D} \\rightarrow{} R^{D_{Num Classes}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The shared MLP implementation \"trick\":\n",
    "\n",
    "We can represent the dims $D$ as 2D image channels $C$\n",
    "* Input: $R^{1\\times N\\times {C_{in}}}$\n",
    "* Layers: 2D convolution with output size of $1\\times 1 \\times {C_{out}}$, followed by activation layer.\n",
    "Where $C_{out}$ is the number of filters, each has size of $1\\times 1 \\times {C_{in}}$, simmilar to $1D$ fully connected layer.\n",
    "* Output: $R^{1\\times N\\times {C_{out}}}$ -->\n",
    "Shared mlp implementation \"trick\":\n",
    "* Use conv layers :  Number of filters $C_{out}$, each filter size is  ${ 1 \\times {C_{in}}} $.\n",
    "* Input: $R^{ N\\times {C_{in}}}$\n",
    "* Output: $R^{N\\times {C_{out}}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MLP:\n",
    "$$h: R^{C_{in}}\\rightarrow{} R^{C_1}\\rightarrow{} \\dots \\rightarrow{} R^{C_{out}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daliha, please fix the imports, I can't run the code cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class PointNetFeatureExtractor(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 3,\n",
    "                 feat_size: int = 1024,\n",
    "                 layer_dims: Iterable[int] = [64, 128],\n",
    "                 global_feat: bool = True,\n",
    "                 activation=F.relu,\n",
    "                 batchnorm: bool = True,\n",
    "                 transposed_input: bool = False):\n",
    "        super(PointNetFeatureExtractor, self).__init__()\n",
    "\n",
    "        if not isinstance(in_channels, int):\n",
    "            raise TypeError('Argument in_channels expected to be of type int. '\n",
    "                            'Got {0} instead.'.format(type(in_channels)))\n",
    "        if not isinstance(feat_size, int):\n",
    "            raise TypeError('Argument feat_size expected to be of type int. '\n",
    "                            'Got {0} instead.'.format(type(feat_size)))\n",
    "        if not hasattr(layer_dims, '__iter__'):\n",
    "            raise TypeError('Argument layer_dims is not iterable.')\n",
    "        for idx, layer_dim in enumerate(layer_dims):\n",
    "            if not isinstance(layer_dim, int):\n",
    "                raise TypeError('Elements of layer_dims must be of type int. '\n",
    "                                'Found type {0} at index {1}.'.format(\n",
    "                                    type(layer_dim), idx))\n",
    "        if not isinstance(global_feat, bool):\n",
    "            raise TypeError('Argument global_feat expected to be of type '\n",
    "                            'bool. Got {0} instead.'.format(\n",
    "                                type(global_feat)))\n",
    "\n",
    "        # Store feat_size as a class attribute\n",
    "        self.feat_size = feat_size\n",
    "\n",
    "        # Store activation as a class attribute\n",
    "        self.activation = activation\n",
    "\n",
    "        # Store global_feat as a class attribute\n",
    "        self.global_feat = global_feat\n",
    "\n",
    "        # Add in_channels to the head of layer_dims (the first layer\n",
    "        # has number of channels equal to `in_channels`). Also, add\n",
    "        # feat_size to the tail of layer_dims.\n",
    "        if not isinstance(layer_dims, list):\n",
    "            layer_dims = list(layer_dims)\n",
    "        layer_dims.insert(0, in_channels)\n",
    "        layer_dims.append(feat_size)\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        if batchnorm:\n",
    "            self.bn_layers = nn.ModuleList()\n",
    "        for idx in range(len(layer_dims) - 1):\n",
    "            self.conv_layers.append(nn.Conv1d(layer_dims[idx],\n",
    "                                              layer_dims[idx + 1], 1))\n",
    "            if batchnorm:\n",
    "                self.bn_layers.append(nn.BatchNorm1d(layer_dims[idx + 1]))\n",
    "\n",
    "        # Store whether or not to use batchnorm as a class attribute\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.transposed_input = transposed_input\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        r\"\"\"Forward pass through the PointNet feature extractor.\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor representing a pointcloud\n",
    "                (shape: :math:`B \\times N \\times D`, where :math:`B`\n",
    "                is the batchsize, :math:`N` is the number of points\n",
    "                in the pointcloud, and :math:`D` is the dimensionality\n",
    "                of each point in the pointcloud).\n",
    "                If self.transposed_input is True, then the shape is\n",
    "                :math:`B \\times D \\times N`.\n",
    "        \"\"\"\n",
    "        if not self.transposed_input:\n",
    "            x = x.transpose(1, 2)\n",
    "\n",
    "        # Number of points\n",
    "        num_points = x.shape[2]\n",
    "\n",
    "        # By default, initialize local features (per-point features)\n",
    "        # to None.\n",
    "        local_features = None\n",
    "\n",
    "        # Apply a sequence of conv-batchnorm-nonlinearity operations\n",
    "\n",
    "        # For the first layer, store the features, as these will be\n",
    "        # used to compute local features (if specified).\n",
    "        if self.batchnorm:\n",
    "            x = self.activation(self.bn_layers[0](self.conv_layers[0](x)))\n",
    "        else:\n",
    "            x = self.activation(self.conv_layers[0](x))\n",
    "        if self.global_feat is False:\n",
    "            local_features = x\n",
    "\n",
    "        # Pass through the remaining layers (until the penultimate layer).\n",
    "        for idx in range(1, len(self.conv_layers) - 1):\n",
    "            if self.batchnorm:\n",
    "                x = self.activation(self.bn_layers[idx](\n",
    "                    self.conv_layers[idx](x)))\n",
    "            else:\n",
    "                x = self.activation(self.conv_layers[idx](x))\n",
    "\n",
    "        # For the last layer, do not apply nonlinearity.\n",
    "        if self.batchnorm:\n",
    "            x = self.bn_layers[-1](self.conv_layers[-1](x))\n",
    "        else:\n",
    "            x = self.conv_layers[-1](x)\n",
    "\n",
    "        # Max pooling.\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, self.feat_size)\n",
    "\n",
    "        # If extracting global features, return at this point.\n",
    "        if self.global_feat:\n",
    "            return x\n",
    "\n",
    "        # If extracting local features, compute local features by\n",
    "        # concatenating global features, and per-point features\n",
    "        x = x.view(-1, self.feat_size, 1).repeat(1, 1, num_points)\n",
    "        return torch.cat((x, local_features), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Code source: <a href=\"https://github.com/NVIDIAGameWorks/kaolin\">Kaolin</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PointNet Classification Network\n",
    "\n",
    "<img src=\".\\assets\\tut_09_pn10.JPG\" style=\"width:800px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles R., et al. \"PointNet: Deep learning on point sets for 3d classification and segmentation.\" CVPR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Results - Classification\n",
    "<img src=\".\\assets\\tut_09_pn12.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles R., et al. \"PointNet: Deep learning on point sets for 3d classification and segmentation.\" CVPR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PointNet Segmentation Network\n",
    "<img src=\".\\assets\\tut_09_pn11.JPG\" style=\"width:800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles R., et al. \"PointNet: Deep learning on point sets for 3d classification and segmentation.\" CVPR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Semantic Scene Parsing\n",
    "<img src=\".\\assets\\tut_09_pn13.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles R., et al. \"PointNet: Deep learning on point sets for 3d classification and segmentation.\" CVPR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Results - Robustness to Missing Data\n",
    "* Why is PointNet so robust to missing data?\n",
    "<img src=\".\\assets\\tut_09_pn14.JPG\" style=\"width:500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles R., et al. \"PointNet: Deep learning on point sets for 3d classification and segmentation.\" CVPR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Visualizing Global  Point Cloud Features\n",
    "<img src=\".\\assets\\tut_09_pn16.JPG\" style=\"width:800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Visualize What is Learned by Reconstruction\n",
    "* Salient points are discovered!\n",
    "<img src=\".\\assets\\tut_09_pn15.png\" style=\"width:800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles R., et al. \"PointNet: Deep learning on point sets for 3d classification and segmentation.\" CVPR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Point function visualization\n",
    "For each per-point function $h$ (mlp), calculate the values of $h(p)$ for all the points $p$ in the cube.\n",
    "<img src=\".\\assets\\tut_09_pn19.JPG\" style=\"width:800px\">\n",
    "\n",
    "* Semi-equivalent to filter response in CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles R., et al. \"PointNet: Deep learning on point sets for 3d classification and segmentation.\" CVPR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <img src=\"https://img.icons8.com/plasticine/100/000000/not-applicable.png\" style=\"height:50px;display:inline\"> Limitations of PointNet\n",
    "<img src=\".\\assets\\tut_09_pn17.JPG\" style=\"width:800px\">\n",
    "\n",
    "* No local context for each point\n",
    "* Global feature depends on **absolute** coordinate. Hard to generalize to unseen scene configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Points in Metric Space\n",
    "* Learn “kernels” in 3D space and conduct convolution\n",
    "* Kernels have compact spatial support\n",
    "* For convolution, we need to find neighboring points\n",
    "* Possible strategies for range query\n",
    "    * Ball query (results in more stable features)\n",
    "    * k-NN query (faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/plasticine/100/000000/approve-and-update.png\" style=\"height:50px;display:inline\"> PointNet v2.0: Multi-Scale PointNet\n",
    "\n",
    "<img src=\".\\assets\\tut_09_pn18.png\" style=\"width:800px\">\n",
    "\n",
    "Repeated layers:\n",
    "* Sample anchor points\n",
    "* Find neighborhood of anchor points\n",
    "* Apply PointNet in each neighborhood to mimic convolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi, Charles Ruizhongtai, et al. \"Pointnet++: Deep hierarchical feature learning on point sets in a metric space.\" Advances in neural information processing systems. 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Point Clouds DL solutions:\n",
    "* 3DmFV\n",
    "* Dynamic Graph CNN\n",
    "* PCNN\n",
    "* PointCNN\n",
    "* KPConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/bubbles/50/000000/list.png\" style=\"height:50px;display:inline\"> 3D Deep Learning Applications\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Calssification (V)\n",
    "* Semantic segmentation (V)\n",
    "* Part segmentation\n",
    "* Object detection (Upcoming)\n",
    "* Reconstraction \n",
    "* Generation (Upcoming)\n",
    "<!--     * AutoEncoders\n",
    "    * GANs\n",
    "    * Implicit Functions -->\n",
    "* Registration (Upcoming)\n",
    "* Sampling - Downsampling, Upsampling\n",
    "* SLAM\n",
    "* Normal Estimation\n",
    "* and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Registration:\n",
    "Problem statment: Find the rotation and translation transformation between objects\n",
    "\n",
    "<img src=\".\\assets\\tut_09_pnlk1.JPG\" style=\"width:800px\">\n",
    "\n",
    "* PointNetLK (blue) - Deep Learning, based on Lucas–Kanade method (Tracking lecture)\n",
    "    * Comparing 2 point clouds using PointNet features\n",
    "* ICP (orange) - Classic registration method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\".\\assets\\tut_09_pnlk2.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Aoki, Yasuhiro, et al. \"PointNetLK: Robust & efficient point cloud registration using PointNet.\" CVPR 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Generation\n",
    "Conditional generation\n",
    "<img src=\".\\assets\\tut_09_pngen1.JPG\" style=\"width:800px\">\n",
    "Free generation\n",
    "<img src=\".\\assets\\tut_09_pngen2.JPG\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Learning Representations and Generative Models for 3D Point Clouds\n",
    "* FC layer as generator\n",
    "* PointNet as discriminator\n",
    "<img src=\".\\assets\\tut_09_pngen4.png\" style=\"width:300px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\".\\assets\\tut_09_pngen3.png\" style=\"width:600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Achlioptas et al., “Learning Representations and Generative Models for 3D Point Clouds”, ICML 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "More generation methods:\n",
    "* AtlasNet\n",
    "* FoldingNet\n",
    "* PointFlow\n",
    "* OccupancyNetworks\n",
    "* DeepSDF\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Detection:\n",
    "* Generate object proposals from a view (e.g., using SSD)\n",
    "* Recognize using PointNet\n",
    "<img src=\".\\assets\\tut_09_pndet1.JPG\" style=\"width:800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Qi et al., “Frustum PointNets for 3D Object Detection from RGB-D Data”, CVPR 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/clouds/100/000000/hand-tools.png\" style=\"height:50px;display:inline\"> Recommended Tools\n",
    "---\n",
    "Python:\n",
    "* Open3D\n",
    "* trimesh\n",
    "* Ipyvolume - Visualization for Notebooks\n",
    "\n",
    "Deep Learning:\n",
    "* Python3D\n",
    "* Kaolin (Pytorch)\n",
    "* TensorFlow Graphics\n",
    "\n",
    "Visualize:\n",
    "* CloudCompare\n",
    "* MeshLab\n",
    "\n",
    "For more 3D deep learnig frameworks and datasets:\n",
    "* <a href=\"https://github.com/Yochengliu/awesome-point-cloud-analysis\">awesome-point-cloud-analysis</a>\n",
    "* <a href=\"https://github.com/timzhang642/3D-Machine-Learning#datasets\">3D-Machine-Learning#datasets</a> \n",
    "\n",
    "Datasets:\n",
    "* ModelNet\n",
    "* ShapeNet\n",
    "* PartNet\n",
    "* Sydney Urban Opject DAtaset\n",
    "* Stanford 3D\n",
    "* KITTI\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "* 3D Deep Learning\n",
    "    * General (Both highly recomanded):\n",
    "        *  <a href=\"https://www.youtube.com/watch?time_continue=6&v=vfL6uJYFrp4&feature=emb_logo\">3D Deep Learning Tutorial from SU lab at UCSD</a> - Hao Su\n",
    "        *  <a href=\"https://www.youtube.com/watch?v=wLU4YsC_4NY\n",
    "o\">Geometric deep learning</a> - Micahel Bronstein\n",
    "    * <a href=\"https://www.youtube.com/watch?v=Cge-hot0Oc0&t=24s\">PointNet</a> \n",
    "    * <a href=\"https://www.youtube.com/watch?v=HIUGOKSLTcE\">3DmFV</a>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "----\n",
    "* Slides - <a href=\"http://www.itzikbs.com/category/research-blog\">Yizhak (Itzik) Ben-Shabat</a>,  <a href=\"https://ci2cv.net/people/simon-lucey/\">Simon Lucey (CMU)</a>, <a href=\"https://cseweb.ucsd.edu/~haosu/\">Hao Su, Jiayuan Gu and Minghua Liu(UCSanDiego) </a>\n",
    "* Multiple View Geometry in Computer Vision - Hartley and Zisserman - Sections 9,10\n",
    "* <a href=\"https://www.springer.com/gp/book/9781848829343\">Computer Vision: Algorithms and Applications</a> - Richard Szeliski - Sections 11,12\n",
    "\n",
    "* Icons from <a href=\"https://icons8.com/\">Icon8.com</a> - https://icons8.com\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}